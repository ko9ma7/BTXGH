{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver \n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "path = 'c:/chromedriver_win32/chromedriver.exe'\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "#for url in urlList:\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "soup = bs(html, 'lxml')\n",
    "    \n",
    "item = {}  \n",
    "    \n",
    "item['manufacturer'] = soup.select_one('div.goods_info > div:nth-of-type(1) > span:nth-of-type(1) > em').text # 제조사\n",
    "item['brand'] =  soup.select_one('div.goods_info > div:nth-of-type(1) > span:nth-of-type(2) > em').text # 브랜드\n",
    "item['heart'] = soup.select_oner('#jjim > em.cnt._keepCount').text # 찜하기 수 \n",
    "\n",
    "# 평점별 리뷰개수\n",
    "p = re.compile(r'(?!=[(])\\d+(?=[)])')\n",
    "item['total_review'] = int(p.search(soup.select_one('#_score_filter > li:nth-of-type(1)').get_text()).group())\n",
    "item['review_score_5'] = int(p.search(soup.select_one('#_score_filter > li:nth-of-type(2)').get_text()).group())\n",
    "item['review_score_4'] = int(p.search(soup.select_one('#_score_filter > li:nth-of-type(3)').get_text()).group())\n",
    "item['review_score_3'] = int(p.search(soup.select_one('#_score_filter > li:nth-of-type(4)').get_text()).group())\n",
    "item['review_score_2'] = int(p.search(soup.select_one('#_score_filter > li:nth-of-type(5)').get_text()).group())\n",
    "item['review_score_1'] = int(p.search(soup.select_one('#_score_filter > li:nth-of-type(6)').get_text()).group())\n",
    "\n",
    "page = soup.select_one('#_review_paging > a.next_end')\n",
    "if page:\n",
    "    p = re.compile(r'(?!=page.)\\d+(?=,)')\n",
    "    lastPage = int(p.search(str(page)).group()) # 총 페이지 개수\n",
    "else:\n",
    "    lastPage = int(soup.select('#_review_paging > a')[-1].text) # 다음 페이지 버튼 없는 경우\n",
    "\n",
    "name = []\n",
    "path = []\n",
    "star = []\n",
    "date = []\n",
    "subject = []   \n",
    "content = []\n",
    "img = []\n",
    "\n",
    "#driver.find_element_by_css_selector('#_review_sort_select > span:nth-child(2) > a').click() # 리뷰 최신순\n",
    "\n",
    "page = 1\n",
    "while page<=lastPage: #보고 싶은 페이지 수 만큼\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = bs(html,'lxml')\n",
    "    \n",
    "    name.extend([w.get_text() for w in soup.select('div.avg_area > span.info > span.name')]) # 작성자 \n",
    "    path.extend([w.get_text() for w in soup.select('div.avg_area > span.info > span.path')]) # 구매처\n",
    "    star.extend([w.get_text() for w in soup.select('div.avg_area > a > span.curr_avg')[2:]]) # 별점\n",
    "    date.extend([w.get_text() for w in soup.select('div.avg_area > span.date')]) # 작성일\n",
    "    subject.extend([w.get_text() for w in soup.find_all(class_='subjcet')]) # 제목 텍스트\n",
    "    content.extend([w.get_text().replace('\\xa0', '') for w in soup.find_all(class_='atc')]) # 본문 텍스트\n",
    "        \n",
    "    for s in soup.find_all(class_='atc_area'): # 이미지 유무\n",
    "        if 'img_box' in str(s):\n",
    "            img.append('Y')\n",
    "        else:\n",
    "            img.append('N')            \n",
    "    \n",
    "    #page.click() # error -> 클릭할 버튼을 specific하게 지정 필요\n",
    "    if page < lastPage:\n",
    "        if lastPage > 10:\n",
    "            driver.find_elements_by_xpath('//*[@id=\"_review_paging\"]/a')[:-1][page-1].click()\n",
    "        #pageList = driver.find_elements_by_xpath('//*[@id=\"_review_paging\"]/a')\n",
    "        else:\n",
    "            driver.find_elements_by_xpath('//*[@id=\"_review_paging\"]/a')[page-1].click() # 다음 페이지\n",
    "    page+=1    \n",
    "\n",
    "item['name'] = name\n",
    "item['path'] = path\n",
    "item['star'] = star\n",
    "item['date'] = date\n",
    "item['subject'] = subject\n",
    "item['content'] = content\n",
    "item['image'] = img  \n",
    "\n",
    "df = pd.concat([df, pd.DataFrame(item)])\n",
    "\n",
    "#urlList for문 끝\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "\n",
    "print(\"--- %.4f seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates().shape[0] == int(soup.select_one('#fixed_tab_area > div > ul > li.mall_review.on > a > em').text) # 리뷰수 일치하는지"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
